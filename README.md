# ğŸ“Š Benchmarked Annotation Dataset for LLM-Based Information Extraction

## ğŸ”’ Repository Access
This is a **private repository** containing a **benchmarked and curated dataset** used for evaluating and comparing multiple LLM prompting strategies for structured information extraction from scientific literature.

The repository is intended for **internal use and controlled collaboration only**.

---

## ğŸ“Œ Overview

This repository contains:

- **Documented prompt specifications** (baseline, GA-focused, and final refined prompts)
- A **fully annotated benchmark dataset** (170 files)
- Outputs from **multiple LLM prompting strategies**
- **Comparison, merging, and validation scripts**
- Interactive **HTML-based visualization tools** for schema exploration and annotation inspection
- A visual **workflow of the benchmarking pipeline**

The dataset enables:
- Systematic comparison of *initial*, *refined*, and *GA-based* prompts
- Quantitative mismatch analysis between LLM outputs and a reference database
- Reproducible benchmarking and error analysis

---

## ğŸ“‚ Repository Structure

```
â”œâ”€â”€ Annotated_Files/
â”‚   â””â”€â”€ all_annotations.json
â”‚
â”œâ”€â”€ Images/
â”‚   â””â”€â”€ benchmarking_workflow.html
â”‚
â”œâ”€â”€ JSON_Files/
â”‚   â”œâ”€â”€ JSON_Firstprompt_files/
â”‚   â”œâ”€â”€ JSON_Finalprompt_files/
â”‚   â”œâ”€â”€ JSON_GAprompt_files/
â”‚   â””â”€â”€ JSON_Combined_files/
â”‚
â”œâ”€â”€ Prompts/
â”‚   â”œâ”€â”€ First Prompt.docx
â”‚   â”œâ”€â”€ GA Prompt.docx
â”‚   â””â”€â”€ Final Prompt.docx
â”‚
â”œâ”€â”€ Python_Scripts/
â”‚   â”œâ”€â”€ combine_summary.py
â”‚   â”œâ”€â”€ mismatch_checker.py
â”‚   â”œâ”€â”€ merge_json_file.py
â”‚   â””â”€â”€ review_newversion.py
â”‚
â”œâ”€â”€ Schema_Viewer/
â”‚   â””â”€â”€ schema_viewer.html
â”‚
â””â”€â”€ README.md
```

---

## 1ï¸âƒ£ Annotated Files (Combined)

**Path:**  
`Annotated_Files/all_annotations.json`

**Description:**  
- Contains **all 170 annotated JSON files merged into a single file**
- Each document retains its **full annotation structure**
- Designed for **direct import into any annotation viewer** (e.g., Label Studio or compatible tools)

**Usage:**  
- Download the file  
- Import into your annotation/inspection tool  
- Instantly browse all documents and their complete annotations  

---

## 2ï¸âƒ£ Images (Benchmarking Workflow)

**Path:**  
`Images/benchmarking_workflow.html`

**Description:**  
- Visual overview of the **benchmarking and extraction pipeline**
- Illustrates prompt variants, JSON generation, merging logic, mismatch analysis, and review cycles

---

## 3ï¸âƒ£ JSON Files (Prompt-wise Outputs)

**Path:**  
`JSON_Files/`

### Subfolders

- **JSON_Firstprompt_files/**  
  JSON files produced using the initial baseline prompt.

- **JSON_Finalprompt_files/**  
  JSON files generated using the final refined prompt.

- **JSON_GAprompt_files/**  
  JSON files produced using a Graph-Analysis (GA)â€“focused prompt.

- **JSON_Combined_files/**  
  JSON files created by merging outputs from the initial and GA-based prompts.

---

## 4ï¸âƒ£ Prompts

**Path:**  
`Prompts/`

This folder contains the **exact prompt specifications** used to generate the LLM outputs included in this benchmarked dataset. Each prompt represents a distinct stage in the prompt development and evaluation pipeline.

The prompts are provided as **`.docx` files** to preserve structure, formatting, and explanatory notes.

### ğŸ“„ Prompt Files

#### 4.1 `First Prompt.docx`
- Initial **baseline prompt** used for large-scale information extraction  
- Captures a broad range of metadata with minimal optimization  
- Serves as the starting point for benchmarking LLM performance  

#### 4.2 `GA Prompt.docx`
- Specialized prompt focusing on **Graph Analysis (GA)**â€“related metadata  
- Targets graph-theoretical decisions, parameters, and analytical steps  
- Improves extraction accuracy for GA-specific fields  

#### 4.3 `Final Prompt.docx`
- Final refined and optimized prompt  
- Integrates insights from baseline errors, GA prompt strengths, and mismatch reviews  
- Produces the highest-quality and most consistent extraction outputs  

---

## 5ï¸âƒ£ Python Scripts

**Path:**  
`Python_Scripts/`

### Script Descriptions

- **combine_summary.py**  
  Generates a global mismatch summary and outputs Excel/CSV files.

- **mismatch_checker.py**  
  Compares LLM-extracted JSON files with a reference Excel database.

- **merge_json_file.py**  
  Combines JSON outputs from initial and GA-based prompts.

- **review_newversion.py**  
  Supports review, validation, and correction of mismatches.

---
# ğŸ§­ Schema_Viewer (Interactive Schema Explorer)

## Path
`schema_viewer/schema_viewer.html`


## Description

This folder contains an interactive, web-based schema exploration and report-generation tool developed to support understanding, validation, and manual inspection of the benchmarking schema used in this project.

The schema viewer is designed to bridge the gap between:

- **Abstract annotation schemas**, and  
- **Concrete extraction decisions** made by LLMs and human annotators.

It provides a visual and structured interface to explore **families, groups, steps, and sublabels** used in the benchmark.


## ğŸ”¹Key Functionalities

### Schema Graph (Tab 1)

- Interactive force-directed graph representation of the full benchmarking schema  
- Nodes represent families, groups, steps, and sublabels  
- Click to inspect definitions, relationships, and hierarchy  
- Supports expand/collapse, zooming, and search  
<p align="center">
  <img src="Images/tab1.jpeg" width="800">
</p>

### Annotations Explorer (Tab 2)

- Load Label Studioâ€“compatible JSON files  
- Visualize extracted text with highlighted annotation spans  
- Inspect label distributions and annotation metadata  
<p align="center">
  <img src="Images/tab2.jpeg" width="800">
</p>


### Generate Report (Tab 3)

- Manual, guided report builder aligned with the benchmarking schema  
- Allows users to:
  - Select schema families  
  - Tick present preprocessing steps  
  - Enter step numbers, software, algorithms/types, and parameters  
- Produces a structured, text-based report summarizing preprocessing and analysis decisions  
<p align="center">
  <img src="Images/tab3.jpeg" width="800">
</p>


## ğŸ¯ Purpose in the Benchmarking Pipeline

The `schema_viewer` serves multiple roles:

- **Schema documentation**: makes the full analytical decision space explicit and navigable  
- **Annotation validation**: enables rapid inspection of extracted metadata  
- **Human-in-the-loop review**: supports correction and confirmation of LLM outputs  
- **Reporting aid**: standardizes how preprocessing pipelines are summarized  

This tool is especially useful for:

- Onboarding new collaborators  
- Reviewing edge cases and mismatches  
- Demonstrating the scope and complexity of the benchmarking schema  

## â–¶ï¸ Usage

- Open `schema_viewer.html` in any modern web browser (Chrome/Edge recommended)  
- No server or backend required â€” runs fully client-side  
- Optional: load JSON annotation files in the **Annotations Explorer** tab  

---

## ğŸ” Benchmarking Workflow (Summary)

1. Run LLM extraction with different prompts  
2. Store prompt-specific JSON outputs  
3. Merge selected outputs  
4. Compare against the reference database  
5. Generate mismatch summaries  
6. Review and correct inconsistencies  
7. Produce final benchmark-ready dataset  

---

## ğŸ“ Notes

- All data is **benchmarked against a curated reference**
- Intended for **research and methodological evaluation**
- Redistribution is **not permitted** without explicit authorization

---

## License
See the LICENSE file in this repository.

---

## Contact
Sana Hassan Imam  
GitHub: sanahassanimam  
Email: sana.hassan.imam@uni-oldenburg.de
